#!/usr/bin/env python3
"""
è‚¡ç¥¨è³‡æ–™çˆ¬èŸ²ç¨‹å¼ (Stock Data Web Crawler)
ä½¿ç”¨ Playwright é€²è¡Œç¶²é è‡ªå‹•åŒ–èˆ‡ API è³‡æ–™æ“·å–
é©åˆæ•™å­¸ç”¨é€” - åŠŸèƒ½åˆ†é›¢æ¸…æ¥šã€æ˜“æ–¼ç†è§£
"""

import asyncio
import json
from datetime import datetime, timedelta
from pathlib import Path
from typing import List, Dict, Any, Optional
from playwright.async_api import async_playwright, Browser, Page, Response


# ============================================================================
# 1. è¨­å®šå€ (Configuration Section)
# ============================================================================

class CrawlerConfig:
    """çˆ¬èŸ²è¨­å®šé¡åˆ¥ - é›†ä¸­ç®¡ç†æ‰€æœ‰è¨­å®šåƒæ•¸"""
    
    # ç›®æ¨™è‚¡ç¥¨ä»£ç¢¼åˆ—è¡¨
    STOCK_SYMBOLS: List[str] = ["2330", "2317", "2454"]
    
    # çˆ¬å–æ—¥æœŸç¯„åœ
    START_DATE: str = "2024-01-01"
    END_DATE: str = "2024-01-10"
    
    # ç›®æ¨™ç¶²ç«™ URL
    BASE_URL: str = "https://www.example-stock.com/chart"
    
    # é é¢é¸æ“‡å™¨ (Selectors)
    SELECTORS = {
        "stock_input": "#stock-symbol",
        "date_input": "#date-picker",
        "chart_tab": "#chart-tab",
        "submit_button": "#submit-btn"
    }
    
    # API ç›£è½è¨­å®š
    API_ENDPOINT_PATTERN: str = "**/api/stock/data**"
    
    # è¼¸å‡ºæª”æ¡ˆè¨­å®š
    OUTPUT_DIR: str = "data"
    OUTPUT_FILENAME: str = "stock_data_{date}.json"


# ============================================================================
# 2. ç€è¦½å™¨ç®¡ç† (Browser Management)
# ============================================================================

async def initialize_browser() -> tuple[Browser, Page]:
    """
    åˆå§‹åŒ–ç€è¦½å™¨èˆ‡é é¢
    
    Returns:
        tuple: (browser, page) ç€è¦½å™¨èˆ‡é é¢ç‰©ä»¶
    """
    print("ğŸš€ æ­£åœ¨å•Ÿå‹•ç€è¦½å™¨...")
    
    playwright = await async_playwright().start()
    browser = await playwright.chromium.launch(
        headless=False,  # è¨­ç‚º True å¯åœ¨èƒŒæ™¯åŸ·è¡Œ
        slow_mo=100      # æ”¾æ…¢æ“ä½œé€Ÿåº¦ï¼Œæ–¹ä¾¿è§€å¯Ÿ
    )
    
    # å»ºç«‹æ–°é é¢
    page = await browser.new_page()
    
    # è¨­å®šè¦–çª—å¤§å°
    await page.set_viewport_size({"width": 1920, "height": 1080})
    
    print("âœ… ç€è¦½å™¨å•Ÿå‹•å®Œæˆ")
    return browser, page


async def close_browser(browser: Browser) -> None:
    """
    é—œé–‰ç€è¦½å™¨
    
    Args:
        browser: è¦é—œé–‰çš„ç€è¦½å™¨ç‰©ä»¶
    """
    print("ğŸ”’ æ­£åœ¨é—œé–‰ç€è¦½å™¨...")
    await browser.close()
    print("âœ… ç€è¦½å™¨å·²é—œé–‰")


# ============================================================================
# 3. é é¢æ“ä½œ (Page Operations)
# ============================================================================

async def navigate_to_page(page: Page, url: str) -> None:
    """
    å°èˆªåˆ°æŒ‡å®šé é¢
    
    Args:
        page: Playwright é é¢ç‰©ä»¶
        url: ç›®æ¨™ç¶²å€
    """
    print(f"ğŸŒ æ­£åœ¨å‰å¾€é é¢: {url}")
    await page.goto(url, wait_until="networkidle")
    print("âœ… é é¢è¼‰å…¥å®Œæˆ")


async def switch_to_chart_view(page: Page) -> None:
    """
    åˆ‡æ›åˆ°åœ–è¡¨æª¢è¦–æ¨¡å¼
    
    Args:
        page: Playwright é é¢ç‰©ä»¶
    """
    print("ğŸ“Š åˆ‡æ›åˆ°åœ–è¡¨æª¢è¦–...")
    chart_tab_selector = CrawlerConfig.SELECTORS["chart_tab"]
    
    # ç­‰å¾…å…ƒç´ å‡ºç¾ä¸¦é»æ“Š
    await page.wait_for_selector(chart_tab_selector)
    await page.click(chart_tab_selector)
    
    # ç­‰å¾…åˆ‡æ›å®Œæˆ
    await page.wait_for_timeout(1000)
    print("âœ… å·²åˆ‡æ›åˆ°åœ–è¡¨æª¢è¦–")


async def set_stock_symbol(page: Page, symbol: str) -> None:
    """
    è¨­å®šè‚¡ç¥¨ä»£ç¢¼
    
    Args:
        page: Playwright é é¢ç‰©ä»¶
        symbol: è‚¡ç¥¨ä»£ç¢¼
    """
    print(f"ğŸ“ è¨­å®šè‚¡ç¥¨ä»£ç¢¼: {symbol}")
    stock_input_selector = CrawlerConfig.SELECTORS["stock_input"]
    
    # æ¸…ç©ºè¼¸å…¥æ¡†ä¸¦è¼¸å…¥æ–°ä»£ç¢¼
    await page.fill(stock_input_selector, "")
    await page.fill(stock_input_selector, symbol)
    print(f"âœ… å·²è¨­å®šè‚¡ç¥¨ä»£ç¢¼: {symbol}")


async def set_target_date(page: Page, date_str: str) -> None:
    """
    è¨­å®šç›®æ¨™æ—¥æœŸ
    
    Args:
        page: Playwright é é¢ç‰©ä»¶
        date_str: æ—¥æœŸå­—ä¸² (æ ¼å¼: YYYY-MM-DD)
    """
    print(f"ğŸ“… è¨­å®šæ—¥æœŸ: {date_str}")
    date_input_selector = CrawlerConfig.SELECTORS["date_input"]
    
    # è¨­å®šæ—¥æœŸ
    await page.fill(date_input_selector, date_str)
    
    # é»æ“Šæäº¤æŒ‰éˆ•
    submit_button_selector = CrawlerConfig.SELECTORS["submit_button"]
    await page.click(submit_button_selector)
    
    # ç­‰å¾…è³‡æ–™è¼‰å…¥
    await page.wait_for_timeout(2000)
    print(f"âœ… å·²è¨­å®šæ—¥æœŸ: {date_str}")


# ============================================================================
# 4. Response ç›£è½èˆ‡è³‡æ–™æ”¶é›† (Response Monitoring & Data Collection)
# ============================================================================

class ResponseCollector:
    """Response è³‡æ–™æ”¶é›†å™¨"""
    
    def __init__(self):
        self.collected_data: List[Dict[str, Any]] = []
        self.is_collecting: bool = False
    
    async def start_monitoring(self, page: Page) -> None:
        """
        é–‹å§‹ç›£è½ API Response
        
        Args:
            page: Playwright é é¢ç‰©ä»¶
        """
        print("ğŸ‘‚ é–‹å§‹ç›£è½ API Response...")
        self.is_collecting = True
        
        # è¨»å†Š response äº‹ä»¶è™•ç†å™¨
        page.on("response", self._handle_response)
    
    async def _handle_response(self, response: Response) -> None:
        """
        è™•ç†æ””æˆªåˆ°çš„ Response
        
        Args:
            response: Playwright Response ç‰©ä»¶
        """
        if not self.is_collecting:
            return
        
        # æª¢æŸ¥æ˜¯å¦ç‚ºç›®æ¨™ API
        if CrawlerConfig.API_ENDPOINT_PATTERN.replace("**", "") in response.url:
            print(f"ğŸ¯ æ””æˆªåˆ°ç›®æ¨™ API: {response.url}")
            
            try:
                # è§£æ JSON è³‡æ–™
                data = await response.json()
                self.collected_data.append({
                    "url": response.url,
                    "status": response.status,
                    "data": data,
                    "timestamp": datetime.now().isoformat()
                })
                print(f"âœ… è³‡æ–™æ”¶é›†æˆåŠŸ (å…± {len(self.collected_data)} ç­†)")
            except Exception as e:
                print(f"âŒ è³‡æ–™è§£æå¤±æ•—: {e}")
    
    def stop_monitoring(self) -> None:
        """åœæ­¢ç›£è½"""
        self.is_collecting = False
        print("ğŸ›‘ åœæ­¢ç›£è½ API Response")
    
    def get_collected_data(self) -> List[Dict[str, Any]]:
        """
        å–å¾—æ”¶é›†åˆ°çš„è³‡æ–™
        
        Returns:
            List: æ”¶é›†åˆ°çš„è³‡æ–™åˆ—è¡¨
        """
        return self.collected_data
    
    def clear_data(self) -> None:
        """æ¸…ç©ºæ”¶é›†åˆ°çš„è³‡æ–™"""
        self.collected_data = []


# ============================================================================
# 5. æ—¥æœŸè™•ç†å·¥å…· (Date Utilities)
# ============================================================================

def parse_date(date_str: str) -> datetime:
    """
    è§£ææ—¥æœŸå­—ä¸²
    
    Args:
        date_str: æ—¥æœŸå­—ä¸² (æ ¼å¼: YYYY-MM-DD)
    
    Returns:
        datetime: æ—¥æœŸç‰©ä»¶
    """
    return datetime.strptime(date_str, "%Y-%m-%d")


def is_weekend(date: datetime) -> bool:
    """
    åˆ¤æ–·æ˜¯å¦ç‚ºé€±æœ«
    
    Args:
        date: æ—¥æœŸç‰©ä»¶
    
    Returns:
        bool: True è¡¨ç¤ºé€±æœ«ï¼ŒFalse è¡¨ç¤ºå¹³æ—¥
    """
    # weekday(): 0=Monday, 5=Saturday, 6=Sunday
    return date.weekday() >= 5


def generate_date_range(start_date: str, end_date: str, skip_weekends: bool = True) -> List[str]:
    """
    ç”¢ç”Ÿæ—¥æœŸç¯„åœåˆ—è¡¨ï¼ˆå¯é¸æ“‡è·³éé€±æœ«ï¼‰
    
    Args:
        start_date: èµ·å§‹æ—¥æœŸ (æ ¼å¼: YYYY-MM-DD)
        end_date: çµæŸæ—¥æœŸ (æ ¼å¼: YYYY-MM-DD)
        skip_weekends: æ˜¯å¦è·³éé€±æœ«
    
    Returns:
        List[str]: æ—¥æœŸå­—ä¸²åˆ—è¡¨
    """
    print(f"ğŸ“† ç”¢ç”Ÿæ—¥æœŸç¯„åœ: {start_date} åˆ° {end_date}")
    
    start = parse_date(start_date)
    end = parse_date(end_date)
    
    date_list = []
    current = start
    
    while current <= end:
        # å¦‚æœéœ€è¦è·³éé€±æœ«ï¼Œä¸”ç•¶å¤©æ˜¯é€±æœ«ï¼Œå‰‡è·³é
        if skip_weekends and is_weekend(current):
            print(f"â­ï¸  è·³éé€±æœ«: {current.strftime('%Y-%m-%d')} ({current.strftime('%A')})")
        else:
            date_list.append(current.strftime("%Y-%m-%d"))
        
        # ç§»åˆ°ä¸‹ä¸€å¤©
        current += timedelta(days=1)
    
    print(f"âœ… å…±ç”¢ç”Ÿ {len(date_list)} å€‹æœ‰æ•ˆæ—¥æœŸ")
    return date_list


# ============================================================================
# 6. è³‡æ–™å„²å­˜ (Data Storage)
# ============================================================================

def ensure_output_directory() -> Path:
    """
    ç¢ºä¿è¼¸å‡ºç›®éŒ„å­˜åœ¨
    
    Returns:
        Path: è¼¸å‡ºç›®éŒ„è·¯å¾‘
    """
    output_dir = Path(CrawlerConfig.OUTPUT_DIR)
    output_dir.mkdir(exist_ok=True)
    return output_dir


def save_to_json(data: Dict[str, Any], filename: str) -> None:
    """
    å°‡è³‡æ–™å„²å­˜ç‚º JSON æª”æ¡ˆ
    
    Args:
        data: è¦å„²å­˜çš„è³‡æ–™
        filename: æª”æ¡ˆåç¨±
    """
    print(f"ğŸ’¾ æ­£åœ¨å„²å­˜è³‡æ–™åˆ°: {filename}")
    
    # ç¢ºä¿è¼¸å‡ºç›®éŒ„å­˜åœ¨
    output_dir = ensure_output_directory()
    file_path = output_dir / filename
    
    try:
        # å¯«å…¥ JSON æª”æ¡ˆ (ä½¿ç”¨ç¸®æ’è®“æª”æ¡ˆæ˜“è®€)
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… è³‡æ–™å·²å„²å­˜: {file_path}")
    except Exception as e:
        print(f"âŒ å„²å­˜å¤±æ•—: {e}")


def save_daily_data(date: str, stock_data: List[Dict[str, Any]]) -> None:
    """
    å„²å­˜å–®æ—¥è³‡æ–™
    
    Args:
        date: æ—¥æœŸå­—ä¸²
        stock_data: è‚¡ç¥¨è³‡æ–™åˆ—è¡¨
    """
    filename = CrawlerConfig.OUTPUT_FILENAME.format(date=date)
    
    # çµ„ç¹”è³‡æ–™çµæ§‹
    output_data = {
        "date": date,
        "crawled_at": datetime.now().isoformat(),
        "total_records": len(stock_data),
        "data": stock_data
    }
    
    save_to_json(output_data, filename)


# ============================================================================
# 7. ä¸»æµç¨‹æ§åˆ¶ (Main Flow Control)
# ============================================================================

async def crawl_stock_data_for_date(
    page: Page,
    collector: ResponseCollector,
    stock_symbol: str,
    date: str
) -> List[Dict[str, Any]]:
    """
    çˆ¬å–å–®ä¸€è‚¡ç¥¨åœ¨ç‰¹å®šæ—¥æœŸçš„è³‡æ–™
    
    Args:
        page: Playwright é é¢ç‰©ä»¶
        collector: Response æ”¶é›†å™¨
        stock_symbol: è‚¡ç¥¨ä»£ç¢¼
        date: æ—¥æœŸå­—ä¸²
    
    Returns:
        List: æ”¶é›†åˆ°çš„è³‡æ–™
    """
    print(f"\n{'='*60}")
    print(f"ğŸ“ˆ é–‹å§‹çˆ¬å–: è‚¡ç¥¨ {stock_symbol} / æ—¥æœŸ {date}")
    print(f"{'='*60}")
    
    # æ¸…ç©ºä¹‹å‰çš„è³‡æ–™
    collector.clear_data()
    
    # è¨­å®šè‚¡ç¥¨ä»£ç¢¼
    await set_stock_symbol(page, stock_symbol)
    
    # è¨­å®šæ—¥æœŸï¼ˆé€™æœƒè§¸ç™¼ API è«‹æ±‚ï¼‰
    await set_target_date(page, date)
    
    # ç­‰å¾…è³‡æ–™æ”¶é›†å®Œæˆ
    await page.wait_for_timeout(3000)
    
    # å–å¾—æ”¶é›†åˆ°çš„è³‡æ–™
    collected = collector.get_collected_data()
    print(f"âœ… å®Œæˆçˆ¬å–: æ”¶é›†åˆ° {len(collected)} ç­†è³‡æ–™")
    
    return collected


async def crawl_multiple_stocks(
    page: Page,
    collector: ResponseCollector,
    stock_symbols: List[str],
    dates: List[str]
) -> Dict[str, Dict[str, List[Dict[str, Any]]]]:
    """
    çˆ¬å–å¤šæ”¯è‚¡ç¥¨åœ¨å¤šå€‹æ—¥æœŸçš„è³‡æ–™
    
    Args:
        page: Playwright é é¢ç‰©ä»¶
        collector: Response æ”¶é›†å™¨
        stock_symbols: è‚¡ç¥¨ä»£ç¢¼åˆ—è¡¨
        dates: æ—¥æœŸåˆ—è¡¨
    
    Returns:
        Dict: æ•´ç†å¾Œçš„è³‡æ–™ {date: {symbol: data}}
    """
    all_data = {}
    
    # è¿´åœˆè™•ç†æ¯å€‹æ—¥æœŸ
    for date in dates:
        print(f"\n{'#'*60}")
        print(f"ğŸ“… è™•ç†æ—¥æœŸ: {date}")
        print(f"{'#'*60}")
        
        daily_data = {}
        
        # è¿´åœˆè™•ç†æ¯æ”¯è‚¡ç¥¨
        for symbol in stock_symbols:
            stock_data = await crawl_stock_data_for_date(
                page, collector, symbol, date
            )
            daily_data[symbol] = stock_data
        
        all_data[date] = daily_data
        
        # å„²å­˜ç•¶æ—¥è³‡æ–™
        save_daily_data(date, daily_data)
    
    return all_data


async def main():
    """
    ä¸»ç¨‹å¼é€²å…¥é» - ä¸²æ¥æ‰€æœ‰åŠŸèƒ½æ¨¡çµ„
    """
    print("="*60)
    print("ğŸš€ è‚¡ç¥¨è³‡æ–™çˆ¬èŸ²ç¨‹å¼å•Ÿå‹•")
    print("="*60)
    
    browser = None
    
    try:
        # æ­¥é©Ÿ 1: åˆå§‹åŒ–ç€è¦½å™¨
        browser, page = await initialize_browser()
        
        # æ­¥é©Ÿ 2: å»ºç«‹ Response æ”¶é›†å™¨
        collector = ResponseCollector()
        await collector.start_monitoring(page)
        
        # æ­¥é©Ÿ 3: å‰å¾€ç›®æ¨™ç¶²ç«™
        await navigate_to_page(page, CrawlerConfig.BASE_URL)
        
        # æ­¥é©Ÿ 4: åˆ‡æ›åˆ°åœ–è¡¨æª¢è¦–
        await switch_to_chart_view(page)
        
        # æ­¥é©Ÿ 5: ç”¢ç”Ÿæ—¥æœŸç¯„åœï¼ˆè·³éé€±æœ«ï¼‰
        dates = generate_date_range(
            CrawlerConfig.START_DATE,
            CrawlerConfig.END_DATE,
            skip_weekends=True
        )
        
        # æ­¥é©Ÿ 6: åŸ·è¡Œçˆ¬èŸ²ä½œæ¥­
        all_data = await crawl_multiple_stocks(
            page,
            collector,
            CrawlerConfig.STOCK_SYMBOLS,
            dates
        )
        
        # æ­¥é©Ÿ 7: åœæ­¢ç›£è½
        collector.stop_monitoring()
        
        print("\n" + "="*60)
        print(f"âœ… çˆ¬èŸ²ä½œæ¥­å®Œæˆï¼")
        print(f"ğŸ“Š å…±è™•ç† {len(dates)} å€‹æ—¥æœŸ")
        print(f"ğŸ“ˆ å…±è™•ç† {len(CrawlerConfig.STOCK_SYMBOLS)} æ”¯è‚¡ç¥¨")
        print(f"ğŸ’¾ è³‡æ–™å·²å„²å­˜è‡³ {CrawlerConfig.OUTPUT_DIR} ç›®éŒ„")
        print("="*60)
        
    except Exception as e:
        print(f"\nâŒ åŸ·è¡Œéç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()
        
    finally:
        # æ­¥é©Ÿ 8: æ¸…ç†è³‡æº
        if browser:
            await close_browser(browser)


# ============================================================================
# ç¨‹å¼é€²å…¥é»
# ============================================================================

if __name__ == "__main__":
    # åŸ·è¡Œä¸»ç¨‹å¼
    asyncio.run(main())
